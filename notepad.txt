import sys
from PyQt6.QtWidgets import QApplication, QMainWindow, QWidget, QVBoxLayout, QHBoxLayout, QLabel, QComboBox, \
    QLineEdit, QTextEdit, QRadioButton


class MainWindow(QMainWindow):
    def __init__(self):
        super().__init__()

        self.setWindowTitle("Live Streaming Video Downloader")

        # Create central widget
        central_widget = QWidget()
        self.setCentralWidget(central_widget)

        # Create layout for central widget
        main_layout = QVBoxLayout()
        central_widget.setLayout(main_layout)

        # Add label
        label = QLabel("Live Streaming Video Downloader")
        main_layout.addWidget(label)

        # Create container widget to hold columns
        container = QWidget()
        container_layout = QHBoxLayout()
        container.setLayout(container_layout)
        main_layout.addWidget(container)

        # Left column
        left_column_widget = QWidget()
        left_column_layout = QVBoxLayout()
        left_column_widget.setLayout(left_column_layout)
        container_layout.addWidget(left_column_widget)

        # Dropdown
        dropdown_label = QLabel("Dropdown:")
        left_column_layout.addWidget(dropdown_label)
        dropdown = QComboBox()
        left_column_layout.addWidget(dropdown)

        # Textbox 1
        textbox1_label = QLabel("Textbox 1:")
        left_column_layout.addWidget(textbox1_label)
        textbox1 = QLineEdit()
        left_column_layout.addWidget(textbox1)

        # Textbox 2
        textbox2_label = QLabel("Textbox 2:")
        left_column_layout.addWidget(textbox2_label)
        textbox2 = QLineEdit()
        left_column_layout.addWidget(textbox2)

        # Middle column
        middle_column_widget = QWidget()
        middle_column_layout = QVBoxLayout()
        middle_column_widget.setLayout(middle_column_layout)
        container_layout.addWidget(middle_column_widget)

        # Radiobutton group
        radiobutton_label = QLabel("Radiobutton Group:")
        middle_column_layout.addWidget(radiobutton_label)
        radiobutton1 = QRadioButton("Option 1")
        radiobutton2 = QRadioButton("Option 2")
        radiobutton3 = QRadioButton("Option 3")
        middle_column_layout.addWidget(radiobutton1)
        middle_column_layout.addWidget(radiobutton2)
        middle_column_layout.addWidget(radiobutton3)

        # Right column
        right_column_widget = QWidget()
        right_column_layout = QVBoxLayout()
        right_column_widget.setLayout(right_column_layout)
        container_layout.addWidget(right_column_widget)

        # Textbox 3
        textbox3_label = QLabel("Textbox 3:")
        right_column_layout.addWidget(textbox3_label)
        textbox3 = QLineEdit()
        right_column_layout.addWidget(textbox3)

        # Textbox 4
        textbox4_label = QLabel("Textbox 4:")
        right_column_layout.addWidget(textbox4_label)
        textbox4 = QLineEdit()
        right_column_layout.addWidget(textbox4)


def main():
    app = QApplication(sys.argv)
    window = MainWindow()
    window.show()
    sys.exit(app.exec())


if __name__ == "__main__":
    main()
Here's a detailed analysis of your options, ranked by **scalability, cost, and performance** for querying/joining large Parquet data in S3:

---

### **Top Recommendations**
#### **1. AWS Athena**  
**Why?**  
- **Scalability**: Serverless, automatically parallelizes queries across petabytes of data.  
- **Cost**: Pay-per-query ($$$ per TB scanned). Use partitioning/columnar projection to reduce costs.  
- **Performance**: Optimized for Parquet, supports complex joins, and integrates with AWS Glue for schema management.  
- **Best for**: Ad-hoc queries with infrequent runs and large datasets.  

#### **2. Fargate + DuckDB**  
**Why?**  
- **Scalability**: DuckDB processes data in-memory efficiently (single-node). Use multiple Fargate tasks for parallelism.  
- **Cost**: Low (pay only for vCPU/memory during query execution).  
- **Performance**: DuckDB is faster than Pandas/Polars for joins, supports SQL, and reads Parquet directly from S3.  
- **Best for**: Moderate-sized datasets (fits in container memory) with frequent queries.  

#### **3. AWS Batch + PySpark**  
**Why?**  
- **Scalability**: PySpark distributes processing across clusters. Handles massive datasets.  
- **Cost**: Moderate (EC2/Fargate costs + EMR if used). Optimize with spot instances.  
- **Performance**: Best for complex joins and transformations at scale.  
- **Best for**: Very large datasets requiring distributed computing.  

---

### **Other Options (Ranked)**
#### **4. Presto Client/Platform**  
- Similar to Athena but self-managed. Higher maintenance overhead.  
- Use only if you need full control over Presto tuning.  

#### **5. Databricks**  
- High performance with Delta Lake optimizations but expensive for AWS-native setups.  

#### **6. Fargate + Polars**  
- Faster than Pandas but still single-node. Limited scalability for large joins.  

#### **7. Fargate + PySpark**  
- Overkill unless you need Sparkâ€™s ecosystem. Higher startup time vs. DuckDB.  

#### **8. S3 Select**  
- Limited SQL support (no joins) and high latency for large datasets.  

#### **9. Fargate + Pandas**  
- Memory-heavy and slow for large data. Avoid.  

#### **10. FireDucks**  
- Not a widely recognized tool; likely less optimized than DuckDB.  

---

### **Key Considerations**  
1. **Data Size**:  
   - Small/medium: **DuckDB on Fargate** (lowest cost).  
   - Large: **Athena** or **AWS Batch + PySpark** (distributed processing).  
2. **Query Frequency**:  
   - Frequent: DuckDB or Athena (minimize startup delays).  
3. **Joins Complexity**:  
   - Simple: Athena.  
   - Complex: PySpark (e.g., broadcast joins for skewed data).  
4. **Cost Control**:  
   - Partition data in S3 and compress Parquet files to reduce Athena/Batch costs.  

---

### **Final Decision**  
- **Start with Athena**: Minimal setup, no infrastructure management, and scalable.  
- **If cost becomes an issue**: Switch to **DuckDB on Fargate** for smaller datasets or **AWS Batch + PySpark** for massive data.  

Let me know if you need help designing the workflow!
